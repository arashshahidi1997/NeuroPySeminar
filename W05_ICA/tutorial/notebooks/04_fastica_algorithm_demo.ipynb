{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc31eec",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# FastICA Algorithm Demo\n",
    "\n",
    "We connect FastICA's theory to practice: generate non-Gaussian sources, mix\n",
    "them, recall the fixed-point update, and compare sklearn's implementation to a\n",
    "minimal manual routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa638f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "TOPIC = \"fastica\"\n",
    "FIG_DIR = os.path.join(\"figures\", TOPIC)\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "rng = np.random.default_rng(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f267d",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## FastICA at a glance\n",
    "\n",
    "For whitened data `z`, one fixed-point update for a component `w` is:\n",
    "\n",
    "\\\\[\n",
    "w_{\\\\text{new}} = \\\\mathbb{E}[z\\\\,g(w^\\\\top z)] - \\\\mathbb{E}[g'(w^\\\\top z)]\\\\,w\n",
    "\\\\]\n",
    "\n",
    "After each update we re-normalize and (for multiple components) orthogonalize\n",
    "the rows of `W`. Popular nonlinearities include:\n",
    "\n",
    "* `g(u) = tanh(u)` (robust, logistic-ish model)\n",
    "* `g(u) = u^3` (kurtosis-based)\n",
    "* `g(u) = u * exp(-u^2 / 2)` (gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10cfa2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Generate sources and mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bac8d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def laplace(size: int) -> np.ndarray:\n",
    "    return rng.laplace(0.0, 1 / np.sqrt(2), size)\n",
    "\n",
    "\n",
    "def logistic(size: int) -> np.ndarray:\n",
    "    return rng.logistic(0.0, 1.0, size)\n",
    "\n",
    "\n",
    "n_samples = 6000\n",
    "S = np.column_stack([laplace(n_samples), logistic(n_samples)])\n",
    "A = np.array([[1.0, 2.0], [-1.5, 1.0]])\n",
    "X = (A @ S.T).T\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(S[:, 0], S[:, 1], s=5, alpha=0.35, color=\"#4477aa\")\n",
    "plt.title(\"True sources\")\n",
    "plt.xlabel(\"$s_1$\")\n",
    "plt.ylabel(\"$s_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.savefig(\n",
    "    os.path.join(FIG_DIR, \"sources_example.png\"), dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=5, alpha=0.35, color=\"#cc6677\")\n",
    "plt.title(\"Observed mixtures\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.savefig(\n",
    "    os.path.join(FIG_DIR, \"mixtures_example.png\"), dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796390ad",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## sklearn FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0319af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ica = FastICA(random_state=0, whiten=\"unit-variance\")\n",
    "S_hat = ica.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(S_hat[:, 0], S_hat[:, 1], s=5, alpha=0.35, color=\"#228833\")\n",
    "plt.title(\"Recovered components (sklearn)\")\n",
    "plt.xlabel(\"$\\\\hat{s}_1$\")\n",
    "plt.ylabel(\"$\\\\hat{s}_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.savefig(\n",
    "    os.path.join(FIG_DIR, \"recovered_fastica_sklearn.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def corr_matrix(true_sources: np.ndarray, estimates: np.ndarray) -> np.ndarray:\n",
    "    true_centered = true_sources - true_sources.mean(axis=0, keepdims=True)\n",
    "    est_centered = estimates - estimates.mean(axis=0, keepdims=True)\n",
    "    true_std = true_centered.std(axis=0, keepdims=True)\n",
    "    est_std = est_centered.std(axis=0, keepdims=True)\n",
    "    corr = np.dot(true_centered / true_std, (est_centered / est_std)) / (\n",
    "        true_sources.shape[0] - 1\n",
    "    )\n",
    "    return corr\n",
    "\n",
    "\n",
    "print(\"Correlation (sklearn):\\n\", np.round(corr_matrix(S, S_hat), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360685c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Manual FastICA iteration\n",
    "\n",
    "We whiten `X`, then run a handful of fixed-point steps with `g(u) = tanh(u)`\n",
    "to visualize convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b0184",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def whiten(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    mean = data.mean(axis=0, keepdims=True)\n",
    "    centered = data - mean\n",
    "    cov = np.cov(centered, rowvar=False)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    eps = 1e-6\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals + eps))\n",
    "    whitening = D_inv_sqrt @ eigvecs.T\n",
    "    dewhitening = eigvecs @ np.diag(np.sqrt(eigvals + eps))\n",
    "    Z = (whitening @ centered.T).T\n",
    "    return Z, whitening, dewhitening\n",
    "\n",
    "\n",
    "def symmetric_decorrelation(W: np.ndarray) -> np.ndarray:\n",
    "    U, s, Vt = np.linalg.svd(W, full_matrices=False)\n",
    "    return (U @ np.diag(1.0 / s) @ Vt) @ W\n",
    "\n",
    "\n",
    "def manual_fastica(Z: np.ndarray, max_iter: int = 200, tol: float = 1e-6):\n",
    "    n_components = Z.shape[1]\n",
    "    W = rng.normal(size=(n_components, n_components))\n",
    "    W = symmetric_decorrelation(W)\n",
    "    contrasts = []\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        WX = Z @ W.T\n",
    "        G = np.tanh(WX)\n",
    "        G_prime = 1.0 - np.tanh(WX) ** 2\n",
    "        W_new = (G.T @ Z) / Z.shape[0] - np.diag(G_prime.mean(axis=0)) @ W\n",
    "        W_new = symmetric_decorrelation(W_new)\n",
    "        delta = np.max(np.abs(np.abs(np.diag(W_new @ W.T)) - 1))\n",
    "        W = W_new\n",
    "        contrasts.append(np.mean(np.log(np.cosh(WX))))\n",
    "        if delta < tol:\n",
    "            break\n",
    "    return W, contrasts\n",
    "\n",
    "\n",
    "Z, whitening, dewhitening = whiten(X)\n",
    "W_manual, contrast_history = manual_fastica(Z)\n",
    "S_manual = Z @ W_manual.T\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(S_manual[:, 0], S_manual[:, 1], s=5, alpha=0.35, color=\"#aa7744\")\n",
    "plt.title(\"Recovered components (manual FastICA)\")\n",
    "plt.xlabel(\"$\\\\tilde{s}_1$\")\n",
    "plt.ylabel(\"$\\\\tilde{s}_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.savefig(\n",
    "    os.path.join(FIG_DIR, \"recovered_fastica_manual.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation (manual):\\n\", np.round(corr_matrix(S, S_manual), 3))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(contrast_history, marker=\"o\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean log cosh (contrast)\")\n",
    "plt.title(\"FastICA convergence\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\n",
    "    os.path.join(FIG_DIR, \"fastica_convergence.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f71ae",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Whitening simplifies ICA to an orthogonal search.\n",
    "* FastICA uses fixed-point updates that do not require learning rates.\n",
    "* Both sklearn and our tiny implementation recover the original non-Gaussian\n",
    "  sources up to permutation/sign."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,scripts//py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "neuropy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
