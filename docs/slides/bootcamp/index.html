<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Sirota Lab ‚Äî Progressive, Reproducible Workflow Bootcamp</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/theme/black.css" id="theme" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/monokai.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js-copycode@1.3.2/plugin/copycode/copycode.css" />
  <style>
    .footnote{position:absolute;bottom:10px;right:20px;font-size:.55em;opacity:.7}
    .reveal h1,.reveal h2{letter-spacing:.5px}
    .reveal section img{border-radius:8px}
    .reveal pre code{font-size:.9em}
    .reveal {
      font-size: 24px; /* default is ~32px */
    }
  </style>
</head>
<body>
  <div class="reveal"><div class="slides">
    <section data-markdown
             data-separator="^\n---\n$"
             data-separator-vertical="^\n--\n$"
             data-separator-notes="^Note:"
             data-charset="utf-8">
      <textarea data-template>

# üß© Lab Reproducible Workflow Tutorial

_(BIDS ‚Üí DataLad ‚Üí Snakemake ‚Üí Integration & FAIR)_  
Sirota Lab Meeting ‚Äî Progressive, reproducible-workflow bootcamp


---

### üß∞ **Setup Conda (Shared vs Personal)**

#### üü¢ **If you already have your own Conda (If not > next slide)**

Don‚Äôt re-initialize ‚Äî just temporarily source the shared one when you need it:

```bash
# Use the shared Conda installation for this session

source /storage/share/python/environments/Anaconda3/etc/profile.d/conda.sh

conda activate cogpy
```

üí° This doesn‚Äôt overwrite your personal Conda.

To return to your own environment, just:

```bash
source ~/.bashrc
```

(or however you normally load your personal Conda).


---

#### ‚öôÔ∏è **If you don‚Äôt have Conda at all**

Initialize your shell using the shared Conda:

```bash

/storage/share/python/environments/Anaconda3/bin/conda init bash

source ~/.bashrc

```

> This will set up Conda automatically every time you open a new shell.

---

**Summary:**

* ‚úÖ Existing Conda users ‚Üí just `source` the shared `conda.sh` when needed.

* üÜï No Conda yet ‚Üí `conda init` with the shared installation once.

**Next**:

clone the BIDS project scaffold:
```
cookiecutter /storage2/arash/codes/Tools/cookiecutter/cookiecutter-bids-extended
```

---

cookicutter output
```bash
Copy code
? project_slug [tutorial-bids]: myproj
Output:

arduino
Copy code
myproj/
‚îú‚îÄ‚îÄ raw/
‚îú‚îÄ‚îÄ sourcedata/
‚îú‚îÄ‚îÄ derivatives/
‚îú‚îÄ‚îÄ code/
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ workflow/
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ python-script.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ matlab-script.m
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ report/
‚îÇ   ‚îî‚îÄ‚îÄ Snakefile
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml
‚îú‚îÄ‚îÄ results/
‚îú‚îÄ‚îÄ dataset_description.json
‚îî‚îÄ‚îÄ README.md
```

---
## **A. BIDS ‚Äì Standardized Data Organization**

### üéØ **Goal**

Learn about BIDS and immediately apply it by BIDS-ifying a dataset.

| Title                                       | Speaker  | Year | Occasion                                                                | Location | Video                                                      | Slides                        |
| ------------------------------------------- | -------- | ---- | ----------------------------------------------------------------------- | -------- | ---------------------------------------------------------- | ----------------------------- |
| BIDS: underlying data management principles | Remi Gau | 2022 | Open Research at the Wellcome Center for Integrative Neuroimaging (WIN) | Online   | [link](https://vimeo.com/showcase/7645853/video/668642973) | [link](https://osf.io/h6gsr/) |

[BEP¬†032: Microelectrode electrophysiology](https://bids.neuroimaging.io/extensions/beps/bep_032.html#bep-032-microelectrode-electrophysiology)

---

### üß† **Theory**

- Introduce the **BIDS standard**: motivation, structure, metadata files.
    
- Explain key elements:
    
    - `sub-*/ses-*` hierarchy
        
    - `dataset_description.json`
        
    - sidecar JSONs
        
    - modality-specific folders (`ecephys`, `motion`, `anat`, `ieeg`, etc.)
        
- Emphasize reproducibility and compatibility with open neuroimaging tools.
    

---
### üíª **Practice**

- Each participant:
    
    - Uses **their own dataset** or a **provided demo dataset**.
        
    - Converts it into **BIDS format**:
        
        - Create folder structure and minimal JSON sidecars.

---

### BIDS Folder Skeleton

```text
project/
‚îî‚îÄ sub-01/
   ‚îî‚îÄ ses-01/
      ‚îú‚îÄ motion/
      ‚îî‚îÄ ecephys/
        ‚îú‚îÄ sub-01_ses-01_ecephys.lfp
        ‚îî‚îÄ sub-01_ses-01_ecephys.json
        
dataset_description.json
```

---

## **B. DataLad ‚Äì Version Control for Data and Collaboration**

### üéØ **Goal**

Learn how to use DataLad to manage datasets, track changes, and share data under the shared lab repository.


---

### üß† **Theory**

- Introduce **DataLad concepts**:
    
    - Git + git-annex integration
        
    - **datasets**, **subdatasets**, **remote storage (RIAs)**
        
    - Provenance tracking and reproducibility
        
- Discuss **collaborative structure** of the **lab repository (`slab`)**.
    

---

### üíª **Practice**

1. **Clone the lab repository**
    

```bash
datalad clone git@server:/path/to/slab
```

---

2. **Add your dataset** as a **subdataset**:
    

```bash
datalad create -d . my_dataset
datalad save -m "added dataset"
```


---

## üß© Your Turn ‚Äî BIDS

> üí° **Hands-on Practice**

- Download a small sample dataset (or use an OpenNeuro example).

```bash
mkdir resources
# datalad install -d . -s ///openneuro/<dataset_id> <shortname>
datalad install -d . -s ///openneuro/ds004598 data/ds-lfplintrack
```

- Organize it into BIDS format (`sub-<subject>/ses-<session>/<modality>/sub-<sub>_ses-<ses>_<modality>.<extension>`).

üïê _Take 5 minutes to complete these steps._

---
# OpenNeuro Datasets

| ID                                                                 | Species           | Modality                    | Type   | Notes                                                        |
| ------------------------------------------------------------------ | ----------------- | --------------------------- | ------ | ------------------------------------------------------------ |
| [ds003463](https://openneuro.org/datasets/ds003463/versions/1.0.2) | Mouse & Rat       | MRI (Mn-enhanced)           | Animal | In vivo MRI for 5√óFAD mice and TgF344-AD rats.               |
| [ds003325](https://openneuro.org/datasets/ds003325/versions/1.0.0) | Mouse             | MRI (T1w)                   | Animal | TDP-43 knock-in mouse model of ALS-FTD.                      |
| [ds006746](https://openneuro.org/datasets/ds006746)                | Mouse             | MRI (Mn2+ enhanced)         | Animal | 3D RARE Mn(II)-enhanced MRI, 24 mice (2 rearing conditions). |
| [ds006670](https://openneuro.org/datasets/ds006670)                | Mouse             | MRI (T1w, T2w)              | Animal | Structural adulthood MRI in C57BL/6J mice.                   |
| [ds004913](https://openneuro.org/datasets/ds004913/versions/1.0.0) | Rat               | fMRI + Optogenetics         | Animal | Optogenetic DBS fMRI in Parkinsonian rats.                   |
| [ds005093](https://openneuro.org/datasets/ds005093/versions/1.0.0) | Non-human Primate | Imaging (PET/MRI)           | Animal | NHP study of microglia activation (TBS course).              |
| [ds000241](https://openneuro.org/datasets/ds000241/versions/00002) | Multiple species  | Imaging (various)           | Animal | ‚ÄúAnimal Kingdom 6 Species‚Äù comparative dataset.              |
| [ds004161](https://openneuro.org/datasets/ds004161)                | Sheep             | MRI / Imaging               | Animal | Turone Sheep Chronic Stress (TSCS) study.                    |
| [ds004598](https://openneuro.org/datasets/ds004598/versions/1.0.0) | Rat               | LFP (electrophysiology)     | Animal | LFP during linear-track task in TgF344-AD rats.              |
| [ds006269](https://openneuro.org/datasets/ds006269)                | Rat               | EEG                         | Animal | 6-hour tethered EEG recordings in Syngap1 rats.              |
| [ds006366](https://openneuro.org/datasets/ds006366/versions/1.0.1) | Mouse             | EEG / Sleep                 | Animal | Mouse Sleep Staging Validation (EEG).                        |
| [ds005688](https://openneuro.org/datasets/ds005688)                | Animal            | Electrophysiology / Optical | Animal | visStim dataset ‚Äì non-MRI animal neurophysiology.            |
| [ds004509](https://openneuro.org/datasets/ds004509/versions/1.0.0) | Rat               | Electrophysiology / Optical | Animal | Visual-deprivation remapping in rats (non-MRI).              |
| [ds005700](https://openneuro.org/datasets/ds005700/versions/1.0.0) | Human             | fMRI                        | Human  | NeuroEmo Emotion Recognition fMRI dataset (~7 GB BIDS).      |
| [ds005126](https://openneuro.org/datasets/ds005126/versions/1.0.0) | Human             | fMRI                        | Human  | ColorSimilarity fMRI study (~36 GB).                         |
| [ds005880](https://openneuro.org/datasets/ds005880/versions/1.0.2) | Human             | fMRI                        | Human  | ‚ÄúDiminished Seventh Chord‚Äù fMRI study.                       |
| [ds004517](https://openneuro.org/datasets/ds004517/versions/1.0.0) | Human             | EEG                         | Human  | EEG dataset for semantic decoding of imagined animals.       |
| [ds004514](https://openneuro.org/datasets/ds004514/versions/1.1.1) | Human             | EEG + fNIRS                 | Human  | Simultaneous EEG/fNIRS recordings.                           |



---

3. **Push your subdataset**:
    

- Option A: use your **own RIA store** on the lab server (local filesystem)
    
- Option B: push directly to the **lab RIA store** (shared bare repo)
    

```bash
datalad push --to origin
```

---

4. **Annotate with metadata** (from BIDS practice)
    

```bash
datalad metadata --set <key>=<value>
datalad save -m "added metadata"
```

---

5. **Sync and verify** updates:
    

```bash
datalad update --merge
```

Everyone can now see that multiple subdatasets have been added by others.

---

**üéÅ Bonus:**  
Use DataLad to fetch a paper from the shared **lab paperpool**:

```bash
datalad get slab/papers/example.pdf
```

---

**Concept Recap Commands**

```bash
datalad create my_dataset
echo "hello" > my_dataset/hello.txt
datalad save -m "add hello"
# explain: datalad create, save, get, push
```

---

### Add as subdataset & push (alt minimal example)

```bash
datalad create -d . my_dataset
datalad save -m "added dataset"
datalad push --to origin
```

---

## üß© Your Turn ‚Äî DataLad

> üí° **Hands-on Practice**

- Run `datalad create test_ds`.
    
- Add and save a small file (`echo "test" > file.txt`).
    
- Explore commands:
    
    - `datalad status`
        
    - `datalad get`
        
    - `datalad push`
        
- Inspect `.git/annex` to see how large files are tracked.
    
- (If access) clone `slab` and add a subdataset on a **feature branch**.
    

üïê _5 minutes ‚Äî experiment and share one useful command!_

---

## **C. Snakemake ‚Äì Workflow Management and Automation**

### üéØ **Goal**

Learn how to define and execute reproducible pipelines operating on BIDS datasets.

Snakemake tutorial slides: https://slides.com/johanneskoester/snakemake-tutorial

---

### üß† **Theory**

- What is a **Snakefile**?
    
- Rules, inputs, outputs, and wildcards.
    
- Workflow visualization (DAGs) and reports.
    
- Integration with version control and DataLad.
    

---

### üíª **Practice**

1. Create a **Snakefile** that includes:
    
    - One **MATLAB script** (dummy computation).
        
    - Two **Python scripts**:
        
        - `calc.py` ‚Üí performs a dummy computation.
            
        - `plot.py` ‚Üí generates a PNG output.
            

---

2. Define outputs under `derivatives/`:
    

```
derivatives/
‚îú‚îÄ‚îÄ dummy-mat/
‚îú‚îÄ‚îÄ dummy-py/
‚îî‚îÄ‚îÄ dummy-png/
```

---

3. Execute the pipeline:
    

```bash
snakemake --cores 2
```

---

4. Visualize workflow:
    

```bash
snakemake --dag | dot -Tpng > dag.png
```

---

5. **Bonus:** Generate a Snakemake report:
    

```bash
snakemake --report report.html
```

---

**Minimal Example Snakefile**

```python
# Snakefile
rule all:
    input: "derivatives/dummy-png/out.png"

rule calc:
    output: "derivatives/dummy-py/out.txt"
    shell: "python code/calc.py > {output}"

rule plot:
    input: "derivatives/dummy-py/out.txt"
    output: "derivatives/dummy-png/out.png"
    shell: "python code/plot.py {input} {output}"
```

---

## üß© Your Turn ‚Äî Snakemake

> üí° **Hands-on Practice**

- Copy or create the sample `Snakefile`.
    
- Run the workflow:
    
    ```bash
    snakemake --cores 2
    ```
    
- Add a **new rule** that writes today‚Äôs date to a file.
    
- Generate a **DAG image** and open the `report.html`.
    

üïê _10 minutes ‚Äî make your workflow produce something new!_

---

## **D. Integration, FAIR Principles & Sustainability**

### üéØ **Goal**

Combine all tools under FAIR principles ‚Äî make workflows reproducible, adaptable, and transparent.

---

### üß† **Theory**

- Summarize:
    
    - **Reproducibility** ‚Äì ‚Äúsame results anytime.‚Äù
        
    - **Adaptability** ‚Äì modular pipelines and reusable code.
        
    - **Transparency** ‚Äì open sharing and provenance tracking.
        
- Reference: _Snakemake ‚ÄúRolling Paper‚Äù_ (FAIR workflow concepts).
    

---

### üíª **Practice**

1. **Make your Snakemake pipeline a CLI tool**:
    
    - Add a `code/` folder.
        
    - Move Snakefile and scripts inside.
        
    - Create a `pyproject.toml` with an **entry point** to run from command line.
        
    - Install into environment (`conda activate labpy`).
        

```bash
pip install -e .
```

---

2. **Run the workflow with provenance tracking**:
    

```bash
datalad run "snakemake --cores 2"
```

---

3. **Save and push results**:
    

```bash
datalad save -m "run workflow with provenance"
datalad push
```

---

4. **Promote your package** into a **DataLad subdataset**:
    

```bash
datalad create -d slab/packages mytool
datalad save -m "added CLI tool package"
```

---

5. **Write documentation** for your tool under `docs/` (can later integrate with MkDocs).
    

---

6. **Push to shared RIA store**:
    

```bash
datalad push --to ria-storage
```

---

7. Perform all steps on a **feature branch** to protect the main `slab` repository.
    

---

**üéÅ Bonus:**

- Inspect your Snakemake log (`snakemake.log` or `.snakemake/log/`).
    
- Email the generated HTML report to **Anton Sirota** directly from the terminal (attach the file or convert to PDF first).
    

---

## üß© Your Turn ‚Äî Integration

> üí° **Hands-on Practice**

- Clone or create a DataLad dataset.
    
- Add your `Snakefile` to the project (ideally under `code/`).
    
- Run your workflow with provenance tracking:
    
    ```bash
    datalad run "snakemake --cores 2"
    ```
    
- View recorded provenance:
    
    ```bash
    datalad run-record show
    ```
    
- Create a **feature branch**, add a brief `README.md`, and push to the **RIA store**.
    

üïê _5‚Äì10 minutes ‚Äî confirm your provenance record works._

---

## FAIR Checklist

- **Findable**: DOI / registered repository
    
- **Accessible**: DataLad + open protocols
    
- **Interoperable**: BIDS format
    
- **Reusable**: Metadata + provenance (Snakemake + DataLad)
    

---
## Why Reproducible Workflows?

- Increasing complexity of neuroimaging analysis
    
- Challenges: sharing, version drift, reruns
    
- Solution: FAIR + modular workflow
    
    - **BIDS**: structure
        
    - **DataLad**: control
        
    - **Snakemake**: automation
        
    - **Integration**: provenance & sharing

---

## üßæ Summary Workflow Overview

1. **BIDSify** ‚Üí make your dataset structured.
    
2. **DataLad** ‚Üí track and share it reproducibly.
    
3. **Snakemake** ‚Üí define and execute workflows.
    
4. **Integrate + FAIR** ‚Üí make it reusable, transparent, and versioned.
    

---

## ‚úÖ End of Tutorial ‚Äî Discussion & Q&A

- What worked smoothly?
    
- What would help you apply this to your own data?
    
- How can we support reproducibility in the lab‚Äôs shared workflows?      </textarea>
    </section>
  </div></div>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/markdown/markdown.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/notes/notes.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/highlight.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js-copycode@1.3.2/plugin/copycode/copycode.js"></script>
  <script>
    Reveal.initialize({
      hash: true,
      slideNumber: true,
      navigationMode: 'linear',
      width: 1280, height: 720, margin: 0.04,
      pdfSeparateFragments: false, pdfMaxPagesPerSlide: 1,
      transition: 'slide', backgroundTransition: 'fade',
      copycode: { button: "hover", display: "icons", timeout: 1200 },
      plugins: [ RevealMarkdown, RevealNotes, RevealHighlight, CopyCode ]
    });
  </script>
</body>
</html>
